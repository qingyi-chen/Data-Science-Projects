{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f492f091-58ff-42b7-a517-43942ed7f8e3",
   "metadata": {},
   "source": [
    "# NLP - Classification and Sentiment Analysis of Reddit Posts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789548fe-fb5c-4075-aa79-47f44fcc1c21",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "Apple and Google are two of the biggest tech giants today. The two companies compete on several fronts: operating systems, browsers, app store, cloud computing etc ([source](https://www.marketingsociety.com/the-library/apple-v-google-they%E2%80%99re-rivals-many-ways-it%E2%80%99s-not-quite-death-match)). \n",
    "\n",
    "The present project aims to levearage on natural language processing to analyze Google and Apple's social media presence, in particular Reddit. The project attempts to understand the contents of discussion on Google and Apple subreddit. The deliverables are a classification model that is able to predict whether a post comes from Google and Apple subreddit, as well as sentiment analysis of users' posts. The insights would shed light on what service and products offered by either company were most talked about in Reddit, and how well received these products and service are among users, hence drive future businesss decision-making in product and service improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f8b2c6-d1b1-4c02-be7c-97cda150b14d",
   "metadata": {},
   "source": [
    "## Part 1: Web API Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523e0dda-ab5f-4cc0-8216-c94657d508d7",
   "metadata": {},
   "source": [
    "Part 1: Web API Data Collection <br>\n",
    "Part 2: Exploratory Data Analysis <br>\n",
    "Part 3: Baseline Classification Models and Zero Shot Classification  <br>\n",
    "Part 4: PyCaret Classification Models <br>\n",
    "Part 5: Sentiment Analysis <br>\n",
    "\n",
    "---\n",
    "\n",
    "In part 1, text data from two subreddits, 'Google' and 'Apple', will be collected using [Pushshift's](https://github.com/pushshift/api) API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ce5e70b-9eea-459b-a3b8-019697c1d060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "353059ae-e77d-4abd-b309-df40ce2b84e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract reddit posts\n",
    "def extract_reddit_posts(epoch_time, subreddit, no_of_extractions):\n",
    "    url =  'https://api.pushshift.io/reddit/search/submission'\n",
    "    all_posts = []\n",
    "    params = {'subreddit':subreddit,'size':250,'before':epoch_time}\n",
    "    \n",
    "    # For loop to repeat the data collection \n",
    "    for i in range(no_of_extractions):\n",
    "        res = requests.get(url,params)\n",
    "        \n",
    "        # Raise error if connection to API fails\n",
    "        if res.status_code != 200:\n",
    "            raise ConnectionError\n",
    "        \n",
    "        # Collect posts and other subreddit data\n",
    "        data = res.json()\n",
    "        posts = data['data']\n",
    "        all_posts += posts\n",
    "        \n",
    "        # Update the epoch time to epoch time of last post in current for loop\n",
    "        params['before'] = posts[-1]['created_utc']\n",
    "        print(f'Extraction {i+1} completed')\n",
    "    \n",
    "    # Convert all extracted data to dataframe\n",
    "    df = pd.DataFrame(all_posts)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d15dfe01-74d8-41e4-ba32-adde52a7ec78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction 1 completed\n",
      "Extraction 2 completed\n",
      "Extraction 3 completed\n",
      "Extraction 4 completed\n",
      "Extraction 5 completed\n",
      "Extraction 6 completed\n",
      "Extraction 7 completed\n",
      "Extraction 8 completed\n",
      "Extraction 9 completed\n",
      "Extraction 10 completed\n",
      "Extraction 11 completed\n",
      "Extraction 12 completed\n",
      "Extraction 13 completed\n",
      "Extraction 14 completed\n",
      "Extraction 15 completed\n",
      "Extraction 16 completed\n",
      "Extraction 17 completed\n",
      "Extraction 18 completed\n",
      "Extraction 19 completed\n",
      "Extraction 20 completed\n",
      "Extraction 21 completed\n",
      "Extraction 22 completed\n",
      "Extraction 23 completed\n",
      "Extraction 24 completed\n",
      "Extraction 25 completed\n",
      "Extraction 26 completed\n",
      "Extraction 27 completed\n",
      "Extraction 28 completed\n",
      "Extraction 29 completed\n",
      "Extraction 30 completed\n",
      "Extraction 31 completed\n",
      "Extraction 32 completed\n",
      "Extraction 33 completed\n",
      "Extraction 34 completed\n",
      "Extraction 35 completed\n",
      "Extraction 36 completed\n",
      "Extraction 37 completed\n",
      "Extraction 38 completed\n",
      "Extraction 39 completed\n",
      "Extraction 40 completed\n",
      "Extraction 41 completed\n",
      "Extraction 42 completed\n",
      "Extraction 43 completed\n",
      "Extraction 44 completed\n",
      "Extraction 45 completed\n",
      "Extraction 46 completed\n",
      "Extraction 47 completed\n",
      "Extraction 48 completed\n",
      "Extraction 49 completed\n",
      "Extraction 50 completed\n",
      "Extraction 51 completed\n",
      "Extraction 52 completed\n",
      "Extraction 53 completed\n",
      "Extraction 54 completed\n",
      "Extraction 55 completed\n",
      "Extraction 56 completed\n",
      "Extraction 57 completed\n",
      "Extraction 58 completed\n",
      "Extraction 59 completed\n",
      "Extraction 60 completed\n",
      "Extraction 61 completed\n",
      "Extraction 62 completed\n",
      "Extraction 63 completed\n",
      "Extraction 64 completed\n",
      "Extraction 65 completed\n",
      "Extraction 66 completed\n",
      "Extraction 67 completed\n",
      "Extraction 68 completed\n",
      "Extraction 69 completed\n",
      "Extraction 70 completed\n",
      "Extraction 71 completed\n",
      "Extraction 72 completed\n",
      "Extraction 73 completed\n",
      "Extraction 74 completed\n",
      "Extraction 75 completed\n",
      "Extraction 76 completed\n",
      "Extraction 77 completed\n",
      "Extraction 78 completed\n",
      "Extraction 79 completed\n",
      "Extraction 80 completed\n"
     ]
    }
   ],
   "source": [
    "# Extract data from 'Apple' subreddit\n",
    "apple_posts = extract_reddit_posts(epoch_time= 1664521047, subreddit = 'apple', no_of_extractions = 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "082ce8bd-70e6-4724-93b3-407f63c5bb73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction 1 completed\n",
      "Extraction 2 completed\n",
      "Extraction 3 completed\n",
      "Extraction 4 completed\n",
      "Extraction 5 completed\n",
      "Extraction 6 completed\n",
      "Extraction 7 completed\n",
      "Extraction 8 completed\n",
      "Extraction 9 completed\n",
      "Extraction 10 completed\n",
      "Extraction 11 completed\n",
      "Extraction 12 completed\n",
      "Extraction 13 completed\n",
      "Extraction 14 completed\n",
      "Extraction 15 completed\n",
      "Extraction 16 completed\n",
      "Extraction 17 completed\n",
      "Extraction 18 completed\n",
      "Extraction 19 completed\n",
      "Extraction 20 completed\n",
      "Extraction 21 completed\n",
      "Extraction 22 completed\n",
      "Extraction 23 completed\n",
      "Extraction 24 completed\n",
      "Extraction 25 completed\n",
      "Extraction 26 completed\n",
      "Extraction 27 completed\n",
      "Extraction 28 completed\n",
      "Extraction 29 completed\n",
      "Extraction 30 completed\n",
      "Extraction 31 completed\n",
      "Extraction 32 completed\n",
      "Extraction 33 completed\n",
      "Extraction 34 completed\n",
      "Extraction 35 completed\n",
      "Extraction 36 completed\n",
      "Extraction 37 completed\n",
      "Extraction 38 completed\n",
      "Extraction 39 completed\n",
      "Extraction 40 completed\n",
      "Extraction 41 completed\n",
      "Extraction 42 completed\n",
      "Extraction 43 completed\n",
      "Extraction 44 completed\n",
      "Extraction 45 completed\n",
      "Extraction 46 completed\n",
      "Extraction 47 completed\n",
      "Extraction 48 completed\n",
      "Extraction 49 completed\n",
      "Extraction 50 completed\n",
      "Extraction 51 completed\n",
      "Extraction 52 completed\n",
      "Extraction 53 completed\n",
      "Extraction 54 completed\n",
      "Extraction 55 completed\n",
      "Extraction 56 completed\n",
      "Extraction 57 completed\n",
      "Extraction 58 completed\n",
      "Extraction 59 completed\n",
      "Extraction 60 completed\n",
      "Extraction 61 completed\n",
      "Extraction 62 completed\n",
      "Extraction 63 completed\n",
      "Extraction 64 completed\n",
      "Extraction 65 completed\n",
      "Extraction 66 completed\n",
      "Extraction 67 completed\n",
      "Extraction 68 completed\n",
      "Extraction 69 completed\n",
      "Extraction 70 completed\n",
      "Extraction 71 completed\n",
      "Extraction 72 completed\n",
      "Extraction 73 completed\n",
      "Extraction 74 completed\n",
      "Extraction 75 completed\n",
      "Extraction 76 completed\n",
      "Extraction 77 completed\n",
      "Extraction 78 completed\n",
      "Extraction 79 completed\n",
      "Extraction 80 completed\n"
     ]
    }
   ],
   "source": [
    "# Extract data from 'Google' subreddit\n",
    "google_posts = extract_reddit_posts(epoch_time= 1664521047, subreddit = 'google', no_of_extractions = 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "435f200a-c2c6-4f48-98f6-e3463ca126e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data\n",
    "apple_posts.to_csv('Scraped Datasets/apple_posts',index=False)\n",
    "google_posts.to_csv('Scraped Datasets/google_posts',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
