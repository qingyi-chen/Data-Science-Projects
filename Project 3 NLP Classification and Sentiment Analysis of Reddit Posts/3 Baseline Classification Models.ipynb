{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fd1782f-e538-4f6e-b407-b4aa472174c5",
   "metadata": {},
   "source": [
    "# NLP - Classification and Sentiment Analysis of Reddit Posts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba007e79-c2f3-492a-916f-e0834cc9d236",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Part 3: Baseline Classification Models and Zero Shot Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0401407d-aa86-4e42-8278-3e7351e5996e",
   "metadata": {},
   "source": [
    "Part 1: Web API Data Collection <br>\n",
    "Part 2: Exploratory Data Analysis <br>\n",
    "Part 3: Baseline Classification Models and Zero Shot Classification <br>\n",
    "Part 4: PyCaret Classification Models <br>\n",
    "Part 5: Sentiment Analysis <br>\n",
    "\n",
    "---\n",
    "\n",
    "In part 3, baseline classification model is explored using different preprocessing. The purpose was to decide on the preprocessing steps that produce the best model performance. Zero shot classification was also experimented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "323b538a-0a68-4fcb-b4e4-070926b4de4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-12 16:59:09.966957: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "#Text processing\n",
    "from nltk import word_tokenize, RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "#Sklearn classification models\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix,get_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "#Zero shot classification\n",
    "from transformers import pipeline\n",
    "\n",
    "#Mlflow\n",
    "import mlflow\n",
    "\n",
    "#Switch off warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419af255-4886-4486-9f96-814169ffa93f",
   "metadata": {},
   "source": [
    "## Import Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2dd3859-4145-4968-bd6b-886ef076d052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36057, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>lemmatised</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>frauenrztin mnchen zentrum  schwangerenvorsorge</td>\n",
       "      <td>google</td>\n",
       "      <td>frauenrztin mnchen zentrum schwangerenvorsorge</td>\n",
       "      <td>frauenrztin mnchen zentrum schwangerenvorsorg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>petition to save stadia i do not have high exp...</td>\n",
       "      <td>google</td>\n",
       "      <td>petition save stadium high expectation i not d...</td>\n",
       "      <td>petit save stadia high expect i not delusion i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>google stadia will be shutting down in january...</td>\n",
       "      <td>google</td>\n",
       "      <td>stadium shutting january purchase will be refu...</td>\n",
       "      <td>stadia shut januari purchas will be refund tec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>google will not let you create an email withou...</td>\n",
       "      <td>google</td>\n",
       "      <td>let create email without phone number</td>\n",
       "      <td>let creat email without phone number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>recover android backup with different resoluti...</td>\n",
       "      <td>google</td>\n",
       "      <td>recover android backup different resolution pa...</td>\n",
       "      <td>recov android backup differ resolut pattern</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text subreddit  \\\n",
       "0    frauenrztin mnchen zentrum  schwangerenvorsorge    google   \n",
       "1  petition to save stadia i do not have high exp...    google   \n",
       "2  google stadia will be shutting down in january...    google   \n",
       "3  google will not let you create an email withou...    google   \n",
       "4  recover android backup with different resoluti...    google   \n",
       "\n",
       "                                          lemmatised  \\\n",
       "0     frauenrztin mnchen zentrum schwangerenvorsorge   \n",
       "1  petition save stadium high expectation i not d...   \n",
       "2  stadium shutting january purchase will be refu...   \n",
       "3              let create email without phone number   \n",
       "4  recover android backup different resolution pa...   \n",
       "\n",
       "                                             stemmed  \n",
       "0      frauenrztin mnchen zentrum schwangerenvorsorg  \n",
       "1  petit save stadia high expect i not delusion i...  \n",
       "2  stadia shut januari purchas will be refund tec...  \n",
       "3               let creat email without phone number  \n",
       "4        recov android backup differ resolut pattern  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('Cleaned Datasets/cleaned_dataset.csv')\n",
    "print(data.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b991b870-9179-4253-8f62-d49403695546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36006, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dropna(axis=0,inplace=True)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a124e23-d153-4168-9e9d-d3cfb2c02fcd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "deec12cd-8c53-4208-9ce6-ddd2667be349",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns='subreddit')\n",
    "y = data['subreddit'].map({'google':0,'apple':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22240ef5-9130-4c82-b514-4b1682f731af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.526607\n",
       "0    0.473393\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac076e1-58a9-4092-aa30-d4736ca9ca3d",
   "metadata": {},
   "source": [
    "The two classes are fairly balanced. Hence, we can use accuracy score subsequently to evaluate model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3cb38a68-ffaf-4728-b566-48a0a078be31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:(27004, 3)\n",
      "test:(9002, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=42,stratify=y)\n",
    "print(f'train:{X_train.shape}')\n",
    "print(f'test:{X_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "874c2145-3545-4850-984c-55fcce6a8e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_stem = X_train['stemmed']\n",
    "X_test_stem = X_test['stemmed']\n",
    "X_train_lemmatised = X_train['lemmatised']\n",
    "X_test_lemmatised = X_test['lemmatised']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0650223d-754a-40cc-baed-2d1481e329b4",
   "metadata": {},
   "source": [
    "Two sets of X_train and X_test were prepared, one was stemmed text, one was lemmatised text. Model performance using these two text datasets will be compared later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf1a569-2acd-4f96-92a2-8a4fb61d6089",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a356b3b2-23d8-4888-9416-ad65c6badb1a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Test Baseline Model with Different Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f81b2d6-2058-4d08-bb02-df4d249fc4f9",
   "metadata": {},
   "source": [
    "Using Logistic Regression as the estimator for baseline model, four combinatons of data preprocessing will be tested:\n",
    "- Lemmatising + Count Vectorizer\n",
    "- Lemmatising + Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "- Stemming + Count Vectorizer\n",
    "- Stemming + Term Frequency-Inverse Document Frequency (TF-IDF)\n",
    "\n",
    "The goal is to decide on the preprocessing steps that give the best model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "4789a94d-bc0f-45a0-b553-4e602faeb20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for CountVectorizer\n",
    "pipe_cvec = Pipeline([\n",
    "    ('cvec',CountVectorizer(max_features=5000)),\n",
    "    ('lr',LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Pipeline for TF-IDF\n",
    "pipe_tfidf = Pipeline([\n",
    "    ('tvec',TfidfVectorizer(max_features=5000)),\n",
    "    ('lr', LogisticRegression(max_iter=1000))\n",
    "])\n",
    "\n",
    "# Function to train model and return four classification metrics\n",
    "def model_train(pipeline,X_train,X_test):\n",
    "    model = pipeline.fit(X_train, y_train)\n",
    "    for metric in ['accuracy','precision','recall','f1']:\n",
    "        print(f'Test {metric}:{get_scorer(metric)(pipeline, X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "f3560142-5e67-4f59-b382-296457a545d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/10/11 23:02:39 INFO mlflow.tracking.fluent: Autologging successfully enabled for sklearn.\n",
      "2022/10/11 23:02:39 INFO mlflow.tracking.fluent: Autologging successfully enabled for tensorflow.\n"
     ]
    }
   ],
   "source": [
    "# Use mlflow to track experiment results\n",
    "mlflow.set_tracking_uri(\"sqlite:///mlflow.db\")\n",
    "mlflow.set_experiment(\"subreddit-posts\")\n",
    "mlflow.autolog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9f094b9a-6900-4a26-ac6a-392465f4223a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.8687597071222543\n",
      "Test precision:0.8907175773535221\n",
      "Test recall:0.8556070826306914\n",
      "Test f1:0.8728093753359853\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run():\n",
    "    model_train(pipe_cvec,X_train_lemmatised,X_test_lemmatised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "3fbd55a3-0d99-4dd2-affe-b30f2687420d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/10/11 00:48:18 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'f50976f1f8804e418c8fb68bc0a7f517', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.8751941424450854\n",
      "Test precision:0.9025583982202448\n",
      "Test recall:0.855185497470489\n",
      "Test f1:0.8782335750622362\n"
     ]
    }
   ],
   "source": [
    "model_train(pipe_tfidf,X_train_lemmatised,X_test_lemmatised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e251d77b-c260-4ab7-9375-8e75caa22ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/10/11 00:48:25 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'c80f2a1539b04bd693689f5215d4f10f', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.8708675393831817\n",
      "Test precision:0.8891304347826087\n",
      "Test recall:0.862141652613828\n",
      "Test f1:0.8754280821917809\n"
     ]
    }
   ],
   "source": [
    "model_train(pipe_cvec,X_train_stem,X_test_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3136025d-3e73-4e70-bac0-90feb3016408",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/10/11 00:48:33 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'f97276fd1fd74f068a98180282d82474', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:0.8747503882848902\n",
      "Test precision:0.9010428222764588\n",
      "Test recall:0.8560286677908938\n",
      "Test f1:0.8779591395524807\n"
     ]
    }
   ],
   "source": [
    "model_train(pipe_tfidf,X_train_stem,X_test_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683a84b1-e36b-4831-b485-342294a58cf0",
   "metadata": {},
   "source": [
    "**Insights from Different Preprocessing** <br>\n",
    "|           Estimator          | Preprocessing                  |   Test Accuracy  |  Test Precision  |    Test Recall   |   Test F1 Score  |\n",
    "|:----------------------------:|--------------------------------|:----------------:|:----------------:|:----------------:|:----------------:|\n",
    "| Logistic Regression          | Lemmatising + Count Vectoriser | 0.8687597071     | 0.8907175774     | 0.8556070826     | 0.8728093753     |\n",
    "| **Logistic Regression**      | **Lemmatising + TF-IDF**       | **0.8751941424** | **0.9025583982** | **0.8551854975** | **0.8782335751** |\n",
    "| Logistic Regression          | Stemming + Count Vectoriser    | 0.8708675394     | 0.8891304348     | 0.8621416526     | 0.8754280822     |\n",
    "| Logistic Regression          | Stemming + TF-IDF              | 0.8747503883     | 0.9010428223     | 0.8560286678     | 0.8779591396     |\n",
    "\n",
    "<br>\n",
    "The combination of lemmatising and TF-IDF produced the best model performance, with highest overall accuracy and f1 score. This means that the model has good accuracy, precision and recall. Hence, this combination will be used for subsequent modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9816d6db-6ebb-46ff-8d9a-cb8f443b5948",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Baseline Model Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "d435b94a-1ed6-4d1b-b882-e719551838fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/10/11 23:01:49 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID 'ec4b270b225f482f9d2a433fe9896ccf', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
      "2022/10/11 23:02:37 INFO mlflow.sklearn.utils: Logging the 5 best runs, 5 runs will be omitted.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;tvec&#x27;,\n",
       "                                        TfidfVectorizer(max_features=5000)),\n",
       "                                       (&#x27;lr&#x27;,\n",
       "                                        LogisticRegression(max_iter=1000))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;tvec__max_features&#x27;: [1000, 2000, 3000, 4000, 5000],\n",
       "                         &#x27;tvec__ngram_range&#x27;: [(1, 1), (1, 2)]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[(&#x27;tvec&#x27;,\n",
       "                                        TfidfVectorizer(max_features=5000)),\n",
       "                                       (&#x27;lr&#x27;,\n",
       "                                        LogisticRegression(max_iter=1000))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;tvec__max_features&#x27;: [1000, 2000, 3000, 4000, 5000],\n",
       "                         &#x27;tvec__ngram_range&#x27;: [(1, 1), (1, 2)]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tvec&#x27;, TfidfVectorizer(max_features=5000)),\n",
       "                (&#x27;lr&#x27;, LogisticRegression(max_iter=1000))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_features=5000)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tvec',\n",
       "                                        TfidfVectorizer(max_features=5000)),\n",
       "                                       ('lr',\n",
       "                                        LogisticRegression(max_iter=1000))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'tvec__max_features': [1000, 2000, 3000, 4000, 5000],\n",
       "                         'tvec__ngram_range': [(1, 1), (1, 2)]})"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_params = {\n",
    "    'tvec__max_features': [1000, 2000, 3000, 4000, 5000],\n",
    "    'tvec__ngram_range': [(1,1), (1,2)]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(pipe_tfidf,\n",
    "                  param_grid = pipe_params,\n",
    "                  cv=5, \n",
    "                  n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train_lemmatised, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "79b99298-2bbe-4d6b-bf85-96c17ef9f923",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tvec__max_features': 5000, 'tvec__ngram_range': (1, 1)}"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "1a372d7d-2334-470c-a3bd-9a5bfb3cbce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0.9027366863905325\n",
      "Test accuracy: 0.8751941424450854\n"
     ]
    }
   ],
   "source": [
    "print(f'Train: {gs.score(X_train_lemmatised, y_train)}')\n",
    "print(f'Test accuracy: {gs.score(X_test_lemmatised, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5c801f-d4e5-4414-9330-bfddb27b4011",
   "metadata": {},
   "source": [
    "Best performing model is the same as the baseline model (maximum features = 5000, unigram)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1b422f-9679-4ab9-b366-26d392e5b7f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc1bafe-ac10-4d61-9f39-9e6ea8d4a4ef",
   "metadata": {},
   "source": [
    "Tree based algorithm, in particular, random forest is explored as an alternative model, because random forest uses bagging and random subspace method to reduce variance in basic tree based model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1670e37-6e2a-4912-a5c2-c40fa44c9a34",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Random Forest with Default Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44453a47-c930-477d-9824-961b302ee36f",
   "metadata": {},
   "source": [
    "Maximum features are reduced to 2000 for random forest since random forest is more computationally expensive compared to logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c060c99c-2cf9-4bc0-83a9-e858ecd228d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing\n",
    "tfidf = TfidfVectorizer(max_features=2000)\n",
    "X_train_lemma_tfidf = tfidf.fit_transform(X_train_lemmatised)\n",
    "X_train_lemma_tfidf_df = pd.DataFrame(X_train_lemma_tfidf.toarray(),columns=tfidf.get_feature_names())\n",
    "\n",
    "X_test_lemma_tfidf = tfidf.fit_transform(X_test_lemmatised)\n",
    "X_test_lemma_tfidf_df = pd.DataFrame(X_test_lemma_tfidf.toarray(),columns=tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "f07b7984-0dac-4eb9-83d0-61b5ffc66248",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/10/11 01:48:39 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '41c53213337e470f991a00b8c6dcc5a9', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.9795118343195266\n",
      "Test score:0.6015087641446638\n"
     ]
    }
   ],
   "source": [
    "# Model training\n",
    "rf = RandomForestClassifier(class_weight='balanced')\n",
    "rf.fit(X_train_lemma_tfidf,y_train)\n",
    "print(f'Train score: {rf.score(X_train_lemma_tfidf_df,y_train)}')\n",
    "print(f'Test score:{rf.score(X_test_lemma_tfidf_df,y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9701d0-39e5-4590-96ef-f961a2f39a1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Random Forest with Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "2d6ac54c-da63-4253-a414-38167481c580",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/10/11 23:14:45 INFO mlflow.utils.autologging_utils: Created MLflow autologging run with ID '0be0dc19a1c24adc9c5b5c2dd2375ee9', which will track hyperparameters, performance metrics, model artifacts, and lineage information for the current sklearn workflow\n",
      "2022/10/11 23:25:20 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed, possibly due older server version. The model artifacts have been logged successfully under ./mlruns/1/0be0dc19a1c24adc9c5b5c2dd2375ee9/artifacts. In addition to exporting model artifacts, MLflow clients 1.7.0 and above attempt to record model metadata to the tracking store. If logging to a mlflow server via REST, consider upgrading the server version to MLflow 1.7.0 or above.\n",
      "2022/10/11 23:25:23 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed, possibly due older server version. The model artifacts have been logged successfully under ./mlruns/1/0be0dc19a1c24adc9c5b5c2dd2375ee9/artifacts. In addition to exporting model artifacts, MLflow clients 1.7.0 and above attempt to record model metadata to the tracking store. If logging to a mlflow server via REST, consider upgrading the server version to MLflow 1.7.0 or above.\n",
      "2022/10/11 23:25:23 INFO mlflow.sklearn.utils: Logging the 5 best runs, 3 runs will be omitted.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;background-color: white;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=RandomForestClassifier(class_weight=&#x27;balanced&#x27;),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;max_depth&#x27;: range(6, 13, 2),\n",
       "                         &#x27;n_estimators&#x27;: [100, 200]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=RandomForestClassifier(class_weight=&#x27;balanced&#x27;),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;max_depth&#x27;: range(6, 13, 2),\n",
       "                         &#x27;n_estimators&#x27;: [100, 200]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(class_weight=&#x27;balanced&#x27;)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(class_weight=&#x27;balanced&#x27;)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=RandomForestClassifier(class_weight='balanced'),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'max_depth': range(6, 13, 2),\n",
       "                         'n_estimators': [100, 200]})"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_param_grid = {\n",
    "    'max_depth': range(6,13,2),\n",
    "    'n_estimators': [100,200]\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(rf,\n",
    "                  param_grid=rf_param_grid,\n",
    "                  n_jobs=-1)\n",
    "\n",
    "gs.fit(X_train_lemma_tfidf_df,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "f201d9b3-7288-44aa-ae0c-08f224e3d58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 12, 'n_estimators': 200}\n"
     ]
    }
   ],
   "source": [
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "525d5902-9d23-4df8-9b13-d58501e91eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.8115014792899409\n",
      "accuracy_score:0.58597736853783\n",
      "precision_score:0.7203832752613241\n",
      "recall_score:0.34865092748735244\n",
      "f1_score:0.4698863636363636\n"
     ]
    }
   ],
   "source": [
    "y_pred = gs.predict(X_test_lemma_tfidf_df)\n",
    "print(f'Train score: {gs.score(X_train_lemma_tfidf_df,y_train)}')\n",
    "\n",
    "for metric in [accuracy_score, precision_score, recall_score, f1_score]:\n",
    "    metric_score = metric(y_test,y_pred)\n",
    "    print(f'{metric.__name__}:{metric_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d0f37ee9-4411-4af6-8baf-1aca584ad155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch off mlflow autologging\n",
    "mlflow.autolog(disable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7300197d-9dab-4cad-92bf-6c6c81eefa86",
   "metadata": {},
   "source": [
    "**Summary of Random Forest Model** <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451485d0-e2c9-41a8-a18b-2bb0c89423d4",
   "metadata": {},
   "source": [
    "|           Estimator          | Preprocessing                  |   Test Accuracy  |  Test Precision  |    Test Recall   |   Test F1 Score  |\n",
    "|:----------------------------:|--------------------------------|:----------------:|:----------------:|:----------------:|:----------------:|\n",
    "| Logistic Regression          | Lemmatising + Count Vectoriser | 0.8687597071     | 0.8907175774     | 0.8556070826     | 0.8728093753     |\n",
    "| Logistic Regression          | Lemmatising + TF-IDF           | 0.8751941424     | 0.9025583982     | 0.8551854975     | 0.8782335751     |\n",
    "| Logistic Regression          | Stemming + Count Vectoriser    | 0.8708675394     | 0.8891304348     | 0.8621416526     | 0.8754280822     |\n",
    "| Logistic Regression          | Stemming + TF-IDF              | 0.8747503883     | 0.9010428223     | 0.8560286678     | 0.8779591396     |\n",
    "| **Random Forest**            | **Lemmatising + TF-IDF**       | **0.5859773685** | **0.7203832753** | **0.3486509275** | **0.4698863636** |\n",
    "- Random forest had poorer accuracy score compared to logistic regression.\n",
    "- Random forest also had issue of overfitting despite reducing maximum depth to address overfitting. This is evident from the much lower accuracy score on test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e09de7-d42b-40fb-ac04-bf0552d5aba7",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Zero Shot Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5783426-44af-4843-9088-6a41d6fd1474",
   "metadata": {},
   "source": [
    "Next, zero shot classification with pre-trained model is tested on the current dataset.<br>\n",
    "<br>\n",
    "The model selected for zero shot classification was [BART model](https://huggingface.co/facebook/bart-large-mnli), which was trained on the Multi-Genre Natural Language Inference (MultiNLI) English corpus, a crowd-sourced collection of 433k sentence pairs annotated with textual entailment information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "8591227a-3f7b-427e-bc1b-4760ec946064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test dataset\n",
    "test_text = pd.concat([X_test['text'],y_test],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "9a8aba25-2503-41c6-888d-ddf853197eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model\n",
    "classifier = pipeline('zero-shot-classification',model='facebook/bart-large-mnli')\n",
    "labels = ['apple','google']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "c96b936a-98af-44a9-b3ea-a9806ffa44ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 9014/9014 [3:47:15<00:00,  1.51s/it]\n"
     ]
    }
   ],
   "source": [
    "# Store results of zero shot classification in 'classifier_results'\n",
    "tqdm.pandas() # Initialize tqdm to activate progress bar\n",
    "test_text['classifier_result'] = test_text.progress_apply(lambda row:classifier(row['text'],labels),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "025d2bbd-552d-47b8-8e84-218c09800cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████| 9014/9014 [00:00<00:00, 26544.08it/s]\n"
     ]
    }
   ],
   "source": [
    "# Tease out y_pred from results dictionary\n",
    "test_text['y_pred'] = test_text.progress_apply(lambda row:row['classifier_result']['labels'][0],axis=1)\n",
    "test_text['y_pred'] = test_text['y_pred'].map({'google':0,'apple':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "a12a0155-dc8f-4e59-b776-efdb6bcbe1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score:0.7164410916352341\n",
      "precision_score:0.9358565737051793\n",
      "recall_score:0.49515177065767285\n",
      "f1_score:0.6476426799007444\n"
     ]
    }
   ],
   "source": [
    "# Test model performance\n",
    "for metric in [accuracy_score, precision_score, recall_score, f1_score]:\n",
    "    metric_score = metric(test_text['subreddit'],test_text['y_pred'])\n",
    "    print(f'{metric.__name__}:{metric_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6639bc-db9f-4850-89e6-de019ba33ade",
   "metadata": {},
   "source": [
    "**Summary of Findings**\n",
    "\n",
    "|           Estimator          | Preprocessing                  |   Test Accuracy  |  Test Precision  |    Test Recall   |   Test F1 Score  |\n",
    "|:----------------------------:|--------------------------------|:----------------:|:----------------:|:----------------:|:----------------:|\n",
    "| Logistic Regression          | Lemmatising + Count Vectoriser | 0.8687597071     | 0.8907175774     | 0.8556070826     | 0.8728093753     |\n",
    "| Logistic Regression          | Lemmatising + TF-IDF           | 0.8751941424     | 0.9025583982     | 0.8551854975     | 0.8782335751     |\n",
    "| Logistic Regression          | Stemming + Count Vectoriser    | 0.8708675394     | 0.8891304348     | 0.8621416526     | 0.8754280822     |\n",
    "| Logistic Regression          | Stemming + TF-IDF              | 0.8747503883     | 0.9010428223     | 0.8560286678     | 0.8779591396     |\n",
    "| Random Forest                | Lemmatising + TF-IDF           | 0.5859773685     | 0.7203832753     | 0.3486509275     | 0.4698863636     |\n",
    "| **Zero Shot Classification** | **-**                          | **0.7164410916** | **0.9358565737** | **0.4951517707** | **0.6476426799** |\n",
    "<br>\n",
    "- Overall, zero shot classification did not outperform baseline logistic regression model. The accuracy score was significantly lower. This was expected since the corpus the model was trained on may not necessarily be relevant for the current text data.\n",
    "- Notably, zero shot classification has very high precision score. This means low type 2 error, and the model is less likely to misclassify posts from Apple subrreddit as Google.\n",
    "- In contrast, recall score is poor. This means high type 1 error, and the model has tendency to misclassify posts from Google subreddits as Apple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17901633-275f-4d15-833b-4d3a52080751",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Export Data for PyCaret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c645dc-f219-47f9-891c-1f3cc8f15fa3",
   "metadata": {},
   "source": [
    "- Data was exported for PyCaret analysis, since PyCaret would be run using a different environment.\n",
    "- Features in the datasets were capped at 2000 instead of 5000 in the baseline model, since PyCaret is computationally expensive when it searched over 16 different classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77df7ce8-4a69-4c09-af05-e89d18ce8c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27004, 2001)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aasp</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>about</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>acabou</th>\n",
       "      <th>acc</th>\n",
       "      <th>accept</th>\n",
       "      <th>access</th>\n",
       "      <th>accessory</th>\n",
       "      <th>...</th>\n",
       "      <th>yet</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "      <th>yous</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yt</th>\n",
       "      <th>zcarsales</th>\n",
       "      <th>zero</th>\n",
       "      <th>zoom</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aasp  ability  able  about  absolutely  acabou  acc  accept  access  \\\n",
       "0   0.0      0.0   0.0    0.0         0.0     0.0  0.0     0.0     0.0   \n",
       "1   0.0      0.0   0.0    0.0         0.0     0.0  0.0     0.0     0.0   \n",
       "2   0.0      0.0   0.0    0.0         0.0     0.0  0.0     0.0     0.0   \n",
       "3   0.0      0.0   0.0    0.0         0.0     0.0  0.0     0.0     0.0   \n",
       "4   0.0      0.0   0.0    0.0         0.0     0.0  0.0     0.0     0.0   \n",
       "\n",
       "   accessory  ...  yet  you  your  yous  youtube   yt  zcarsales  zero  zoom  \\\n",
       "0        0.0  ...  0.0  0.0   0.0   0.0      0.0  0.0        0.0   0.0   0.0   \n",
       "1        0.0  ...  0.0  0.0   0.0   0.0      0.0  0.0        0.0   0.0   0.0   \n",
       "2        0.0  ...  0.0  0.0   0.0   0.0      0.0  0.0        0.0   0.0   0.0   \n",
       "3        0.0  ...  0.0  0.0   0.0   0.0      0.0  0.0        0.0   0.0   0.0   \n",
       "4        0.0  ...  0.0  0.0   0.0   0.0      0.0  0.0        0.0   0.0   0.0   \n",
       "\n",
       "   subreddit  \n",
       "0          0  \n",
       "1          1  \n",
       "2          1  \n",
       "3          1  \n",
       "4          0  \n",
       "\n",
       "[5 rows x 2001 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.concat([X_train_lemma_tfidf_df,y_train.reset_index(drop=True)],axis=1)\n",
    "train_data.to_csv('Cleaned Datasets/train_data.csv',index=False)\n",
    "print(train_data.shape)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "392f7f71-4fce-4375-ad63-2623a518f741",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9002, 2001)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aasp</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>abortion</th>\n",
       "      <th>about</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>ac</th>\n",
       "      <th>acc</th>\n",
       "      <th>accept</th>\n",
       "      <th>access</th>\n",
       "      <th>...</th>\n",
       "      <th>yet</th>\n",
       "      <th>yo</th>\n",
       "      <th>you</th>\n",
       "      <th>young</th>\n",
       "      <th>your</th>\n",
       "      <th>yous</th>\n",
       "      <th>youtube</th>\n",
       "      <th>zcarsales</th>\n",
       "      <th>zoom</th>\n",
       "      <th>subreddit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.214146</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.416624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2001 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   aasp  ability  able  abortion  about  absolutely   ac  acc  accept  \\\n",
       "0   0.0      0.0   0.0       0.0    0.0         0.0  0.0  0.0     0.0   \n",
       "1   0.0      0.0   0.0       0.0    0.0         0.0  0.0  0.0     0.0   \n",
       "2   0.0      0.0   0.0       0.0    0.0         0.0  0.0  0.0     0.0   \n",
       "3   0.0      0.0   0.0       0.0    0.0         0.0  0.0  0.0     0.0   \n",
       "4   0.0      0.0   0.0       0.0    0.0         0.0  0.0  0.0     0.0   \n",
       "\n",
       "     access  ...  yet   yo  you  young  your  yous   youtube  zcarsales  zoom  \\\n",
       "0  0.000000  ...  0.0  0.0  0.0    0.0   0.0   0.0  0.000000        0.0   0.0   \n",
       "1  0.000000  ...  0.0  0.0  0.0    0.0   0.0   0.0  0.000000        0.0   0.0   \n",
       "2  0.214146  ...  0.0  0.0  0.0    0.0   0.0   0.0  0.000000        0.0   0.0   \n",
       "3  0.000000  ...  0.0  0.0  0.0    0.0   0.0   0.0  0.000000        0.0   0.0   \n",
       "4  0.000000  ...  0.0  0.0  0.0    0.0   0.0   0.0  0.416624        0.0   0.0   \n",
       "\n",
       "   subreddit  \n",
       "0          0  \n",
       "1          0  \n",
       "2          1  \n",
       "3          1  \n",
       "4          1  \n",
       "\n",
       "[5 rows x 2001 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.concat([X_test_lemma_tfidf_df,y_test.reset_index(drop=True)], axis=1)\n",
    "test_data.to_csv('Cleaned Datasets/test_data.csv',index=False)\n",
    "print(test_data.shape)\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e031027d-1b4c-4efa-9868-3e815d8b5401",
   "metadata": {},
   "source": [
    "## Summary of Baseline Classification Models and One Shot Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ace981-e90c-47ab-ae30-89fd382cef9e",
   "metadata": {},
   "source": [
    "|         Estimator        | Preprocessing                  |   Test Accuracy  |  Test Precision  |    Test Recall   |   Test F1 Score  |\n",
    "|:------------------------:|--------------------------------|:----------------:|:----------------:|:----------------:|:----------------:|\n",
    "| Logistic Regression      | Lemmatising + Count Vectoriser | 0.8687597071     | 0.8907175774     | 0.8556070826     | 0.8728093753     |\n",
    "| **Logistic Regression**  | **Lemmatising + TF-IDF**       | **0.8751941424** | **0.9025583982** | **0.8551854975** | **0.8782335751** |\n",
    "| Logistic Regression      | Stemming + Count Vectoriser    | 0.8708675394     | 0.8891304348     | 0.8621416526     | 0.8754280822     |\n",
    "| Logistic Regression      | Stemming + TF-IDF              | 0.8747503883     | 0.9010428223     | 0.8560286678     | 0.8779591396     |\n",
    "| Random Forest            | Lemmatising + TF-IDF           | 0.5859773685     | 0.7203832753     | 0.3486509275     | 0.4698863636     |\n",
    "| Zero Shot Classification | -                              | 0.7164410916     | 0.9358565737     | 0.4951517707     | 0.6476426799     |\n",
    "- The text preprocessing that produced the best model performance was lemmatising and Term Frequency-Inverse Document Frequency.\n",
    "- Logistic regression, random forest and one shot classification were tested on the text data. The best performing model was using logistic regression, with an accuracy score of 0.88."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project3kernel",
   "language": "python",
   "name": "project3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
